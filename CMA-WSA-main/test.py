import os
import torch
from torch.utils.data import DataLoader
import numpy as np
from src import Loader, SeqEncoder, ex, Logger
import torchvision.transforms as T
from tqdm import tqdm
import time
import torch.nn.functional as F
from models import CDModel
import copy


def extract_512d_features(model, test_loader, device, save_dir="./Tsne可视化"):
    """
    Extract the 512-dimensional features after passing through linear_classify1 and SiLU activation.
    """
    model.eval()
    features_list = []
    labels_list = []
    
    os.makedirs(save_dir, exist_ok=True)
    
    with torch.no_grad():
        for i, data in tqdm(enumerate(test_loader), total=len(test_loader), desc="提取512维特征"):
            question, answer, image, type_str, mask, image_original = data
            
            # Move data to device
            image = image.to(device)
            question = question.to(device)
            mask = mask.to(device)
            
            # Manually run the model's forward pass and extract features at key points
            # Replicate the forward logic from model.py
            predict_mask = model.maskNet(image)
            if model.use_wavelet_iia:
                predict_mask = model.wavelet_iia_module(predict_mask)
            
            m0 = predict_mask[:, 0, :, :].unsqueeze(1) / 255
            m1 = predict_mask[:, 1, :, :].unsqueeze(1) / 255
            m2 = predict_mask[:, 2, :, :].unsqueeze(1) / 255
            
            # Extract visual features
            v = model.imgModel(pixel_values=image)["pooler_output"]
            v = model.dropout(v)
            v = model.lineV(v)
            v = torch.nn.SiLU()(v)
            
            # Extract text features
            if model.textHead == "siglip_512":
                q = model.textModel(input_ids=question["input_ids"])["pooler_output"]
            elif model.textHead in model.clipList:
                q = model.textModel(**question)["pooler_output"]
                q = model.dropout(q)
                q = model.lineQ(q)
                q = torch.nn.SiLU()(q)
            else:
                q = model.textModel(question)
            
            # Obtain fused features via the router
            moe, gate_prob, importance_loss = model.router(m0, m1, m2, v, q)
            
            # Feature fusion
            fusion_mode = model.config.get('fusion_mode', 'moe').lower()
            if fusion_mode in ['crossattention']:
                x = moe
            else:
                x = torch.mul(moe, q)
            
            x = torch.nn.SiLU()(x)
            x = model.dropout(x)
            
            # Key step: obtain 512-dimensional features
            x = model.linear_classify1(x)  # Obtain 512-dimensional features here
            features_512d = torch.nn.SiLU()(x)  # Apply SiLU activation
            
            # Save features and labels
            features_list.append(features_512d.cpu().numpy())
            labels_list.append(answer.numpy())
    
    # Merge features and labels from all batches
    all_features = np.concatenate(features_list, axis=0)
    all_labels = np.concatenate(labels_list, axis=0)
    
    # Save to files
    feature_file = os.path.join(save_dir, "Feature.txt")
    answer_file = os.path.join(save_dir, "Answer.txt")
    
    np.savetxt(feature_file, all_features)
    np.savetxt(answer_file, all_labels, fmt='%d')
    
    print(f"Features saved to: {feature_file}")
    print(f"Labels saved to: {answer_file}")
    print(f"Feature shape: {all_features.shape}")
    print(f"Label count: {len(all_labels)}")
    
    return all_features, all_labels


def test_model(_config, model, test_loader, test_length, device, logger, wandb_epoch=None, epoch=0, save_features=False):
    v1 = time.time()
    use_wandb = _config["use_wandb"]
    use_moe = _config["MoE"]
    classes = _config["question_classes"]
    num_of_experts = _config["EXPERTS"]
    criterion = torch.nn.CrossEntropyLoss()
    logger.info(f"Testing:")
    
    # If feature saving is enabled, extract features first
    if save_features:
        logger.info("开始提取512维特征...")
        save_dir = "./Tsne可视化"
        extract_512d_features(model, test_loader, device, save_dir)
        logger.info("特征提取完成")
    
    with torch.no_grad():
        model.eval()
        accLoss, maeLoss, rmseLoss, moeLoss, expert_count = 0, 0, 0, 0, np.zeros(num_of_experts)

        countQuestionType = {str(i): 0 for i in range(1, classes + 1)}
        rightAnswerByQuestionType = {str(i): 0 for i in range(1, classes + 1)}

        for i, data in tqdm(
                enumerate(test_loader, 0),
                total=len(test_loader),
                ncols=100,
                mininterval=1,
        ):
            question, answer, image, type_str, mask, image_original = data
            pred, pred_mask, prob, moe_loss = model(
                image.to(device), question.to(device), mask.to(device)
            )

            answer = answer.to(device)
            mae = F.l1_loss(mask.to(device), pred_mask)
            mse = F.mse_loss(mask.to(device), pred_mask)
            rmse = torch.sqrt(mse)
            if prob is not None:
                indices = torch.argmax(prob, dim=-1).cpu()
                counts = torch.bincount(indices, minlength=num_of_experts)
                expert_stats = counts.numpy()
                expert_count = expert_count + expert_stats

            # The ground truth of mask has not been normalized. (Which is intuitively weird)
            # This may be modified in future versions, but currently this method works better than directly normalizing the mask
            if not _config['normalize']:
                mae = mae / 255
                rmse = rmse / 255

            acc_loss = criterion(pred, answer)

            accLoss += acc_loss.cpu().item() * image.shape[0]
            maeLoss += mae.cpu().item() * image.shape[0]
            rmseLoss += rmse.cpu().item() * image.shape[0]
            if use_moe:
                moeLoss += moe_loss.cpu().item() * image.shape[0]
            answer = answer.cpu().numpy()
            pred = np.argmax(pred.cpu().detach().numpy(), axis=1)

            for j in range(answer.shape[0]):
                countQuestionType[type_str[j]] += 1
                if answer[j] == pred[j]:
                    rightAnswerByQuestionType[type_str[j]] += 1

        testAccLoss = accLoss / test_length
        testMaeLoss = maeLoss / test_length
        testRmseLoss = rmseLoss / test_length
        testMoeLoss = moeLoss / test_length
        testLoss = testAccLoss + testRmseLoss + testMaeLoss
        logger.info(
            f"Epoch {epoch} , test loss: {testLoss:.5f}, acc loss : {testAccLoss:.5f}, "
            f"mae loss: {testMaeLoss:.5f}, rmse loss: {testRmseLoss:.5f}"
            f"Expert count: {expert_count}, MoE loss : {testMoeLoss:.5f}"
        )
        numQuestions = 0
        numRightQuestions = 0
        logger.info("Acc:")
        subclassAcc = {}
        accPerQuestionType = {str(i): [] for i in range(1, classes + 1)}
        for type_str in countQuestionType.keys():
            if countQuestionType[type_str] > 0:
                accPerQuestionType[type_str].append(
                    rightAnswerByQuestionType[type_str]
                    * 1.0
                    / countQuestionType[type_str]
                )
            else:
                accPerQuestionType[type_str].append(0)
            numQuestions += countQuestionType[type_str]
            numRightQuestions += rightAnswerByQuestionType[type_str]
            subclassAcc[type_str] = tuple(
                (countQuestionType[type_str], accPerQuestionType[type_str][0])
            )
        logger.info(
            "\t".join(
                [
                    f"{key}({subclassAcc[key][0]}) : {subclassAcc[key][1]:.5f}"
                    for key in subclassAcc.keys()
                ]
            )
        )

        # ave acc
        acc = numRightQuestions * 1.0 / numQuestions
        AA = 0
        for key in subclassAcc.keys():
            if use_wandb:
                if wandb_epoch:
                    wandb_epoch.log({"test " + key + " acc": subclassAcc[key][1]}, step=epoch)
            AA += subclassAcc[key][1]
        AA = AA / len(subclassAcc)

        v2 = time.time()
        logger.info(f"overall acc: {acc:.5f}\taverage acc: {AA:.5f}")
        if wandb_epoch and use_wandb:
            wandb_epoch.log(
                {
                    "test overall acc": acc,
                    "test average acc": AA,
                    "test loss": testLoss,
                    "test acc loss": testAccLoss,
                    "test mae loss": testMaeLoss,
                    "test rmse loss": testRmseLoss,
                    "test moe loss": testMoeLoss,
                    "test time cost": v2 - v1,
                },
                step=epoch,
            )
        
        return {
            "testLoss": testLoss,
            "testAccLoss": testAccLoss,
            "testMaeLoss": testMaeLoss,
            "testRmseLoss": testRmseLoss,
            "testMoeLoss": testMoeLoss,
            "testAcc": acc,
            "testAverageAcc": AA,
            "subclassAcc": subclassAcc,
        }


@ex.automain
def main(_config):
    _config = copy.deepcopy(_config)
    saveDir = _config["saveDir"]
    trainText = _config["trainText"]
    trainImg = _config["trainImg"]
    textHead = _config["textHead"]
    imageHead = _config["imageHead"]
    image_size = _config["image_resize"]
    Data = _config["DataConfig"]
    num_workers = _config["num_workers"]
    pin_memory = _config["pin_memory"]
    persistent_workers = _config["persistent_workers"]
    batch_size = _config["batch_size"]
    
    # Read feature-saving parameters from config
    save_features = _config.get("save_features", False)
    
    if not os.path.exists(saveDir):
        os.mkdir(saveDir)
    log_file_name = (
            saveDir + "Test-" + time.strftime("%Y%m%d-%H%M%S", time.localtime()) + ".log"
    )
    logger = Logger(log_file_name)
    source_img_size = _config["source_image_size"]
    seq_Encoder = SeqEncoder(_config, Data["allQuestionsJSON"], textTokenizer=textHead)
    # RGB
    IMAGENET_MEAN = [0.3833698, 0.39640951, 0.36896593]
    IMAGENET_STD = [0.21045856, 0.1946447, 0.18824594]
    data_transforms = {
        "image": T.Compose(
            [
                T.ToTensor(),
                T.Normalize(IMAGENET_MEAN, IMAGENET_STD),
                T.Resize((image_size, image_size), antialias=True),
            ]
        ),
        "mask": T.Compose(
            [T.ToTensor(), T.Resize((image_size, image_size), antialias=True)]
        ),
    }
    print("Testing dataset preprocessing...")
    test_dataset = Loader(
        _config,
        Data["test"],
        seq_Encoder,
        source_img_size,
        textHead=textHead,
        imageHead=imageHead,
        train=False,
        transform=data_transforms,
    )
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    weightsName = f"{saveDir}lastValModel.pth"
    model = CDModel(
        _config,
        seq_Encoder.getVocab(),
        input_size=image_size,
        textHead=textHead,
        imageHead=imageHead,
        trainText=trainText,
        trainImg=trainImg,
    )
    state_dict = torch.load(weightsName, map_location=device)
    model.load_state_dict(state_dict)
    model.to(device)
    test_length = len(test_dataset)
    test_loader = DataLoader(
        test_dataset,
        batch_size=max(batch_size, 2),
        shuffle=False,
        num_workers=num_workers,
        persistent_workers=persistent_workers,
        pin_memory=pin_memory,
    )
    test_model(_config, model, test_loader, test_length, device, logger, save_features=save_features)
